{
  
    
        "post0": {
            "title": "Similarity Between Images",
            "content": "import matplotlib.pyplot as plt import numpy as np import cv2 from skimage import metrics . original_path = &quot;images/calculator_original.jpg&quot; contrast_path = &quot;images/calculator_contrast.jpg&quot; erased_path = &quot;images/calculator_erased.jpg&quot; failed_path = &quot;images/calculator_failed.jpg&quot; . img_original = cv2.imread(original_path) img_contrast = cv2.imread(contrast_path) img_erased = cv2.imread(erased_path) img_failed = cv2.imread(failed_path) img_original = cv2.cvtColor(img_original,cv2.COLOR_BGR2RGB) img_contrast = cv2.cvtColor(img_contrast, cv2.COLOR_BGR2RGB) img_erased = cv2.cvtColor(img_erased, cv2.COLOR_BGR2RGB) img_failed = cv2.cvtColor(img_failed, cv2.COLOR_BGR2RGB) . #fig = plt.figure(figsize = (15, 15)) #ax1 = fig.add_subplot(131)# 1row + 3cols #ax1 = ax1.imshow(img_original) #plt.colorbar(ax1) #ax2 = fig.add_subplot(132) #ax2 = ax2.imshow(img_contrast) #plt.colorbar(ax2) #ax3 = fig.add_subplot(133) #ax3 = ax3.imshow(img_erased) #plt.colorbar(ax3) . . fig = plt.figure(figsize = (10,10)) images = (&quot;Original&quot;, img_original), (&quot;Contrast&quot;, img_contrast), (&quot;Erased&quot;, img_erased), (&quot;Failed&quot;, img_failed) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 4, i + 1) ax.set_title(name) plt.imshow(image, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) . Compute MSE and SSI . def mse_img(imgA, imgB): # img_orig = original image # img = image that we want to compare against shape = imgA.shape # image shape (height, width, channels) acc = 0 # squared differences of all channels = 0 for i in range(shape[2]): # loop over channels in an img - RGB # calculate the squared differences for each channel in img squared_diff = np.square(imgA[:, :, i] - imgB[:, :, i]) # add squared differences of all channels # this res will be a matrix with same width x height as the original image, but with 1 channel acc = acc + squared_diff #compute the sum of diferences and divide by number of pixels x no of channels return np.sum(acc)/(shape[0] * shape[1]*shape[2]) def ssi_img(imgA, imgB): return metrics.structural_similarity(imgA, imgB, multichannel = True) . def compare_images(imageA, imageB, title): mse = mse_img(imageA, imageB) ssi = ssi_img(imageA, imageB) # setup the figure fig = plt.figure(title) plt.suptitle(&quot;MSE: %.2f, SSIM: %.2f&quot; % (mse, ssi)) # show first image ax = fig.add_subplot(1, 2, 1) plt.imshow(imageA, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) # show the second image ax = fig.add_subplot(1, 2, 2) plt.imshow(imageB, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) # show the images plt.show() . compare_images(img_original, img_original, &quot;Original vs. Original&quot;) . compare_images(img_original, img_contrast, &quot;Original vs. Contrast&quot;) . compare_images(img_original, img_erased, &quot;Original vs. Erased&quot;) . compare_images(img_original, img_failed, &quot;Original vs. Failed&quot;) . References: . https https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/ .",
            "url": "https://simsisim.github.io/sims-pyimage/image%20processing/computer%20vision/2020/12/22/Similarity_Images.html",
            "relUrl": "/image%20processing/computer%20vision/2020/12/22/Similarity_Images.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Day 2 Open CV Tutorial",
            "content": "# Convert jupyter notebook to python script: #!jupyter nbconvert --to script file-name.ipynb #$ python file-name.py --image01 image01.png --image02 image02.png --image03 image03.png # in ap.add_arrgument use : required = True + remove default path #args = vars(ap.parse_args()) . . import numpy as np import cv2 import argparse import matplotlib.pyplot as plt . ap = argparse.ArgumentParser(description=&#39;Fooo&#39;) ap.add_argument(&quot;-i1&quot;, &quot;--image01&quot;, default = &quot;images/face_detection05.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-i2&quot;, &quot;--image02&quot;, default = &quot;images/green.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-i3&quot;, &quot;--image03&quot;, default = &quot;images/tetris_blocks.png&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) args = vars(ap.parse_args([])) #for *.py use args = vars(ap.parse_args()) . 1. Loading / Displaying an image . image = args[&quot;image01&quot;] image = cv2.imread(image)#imread(args[&quot;image&quot;]) (h, w, c) = image.shape for i in range(1): fig = plt.figure(&quot;My Image&quot;, figsize = (7, 5)) ax = fig.add_subplot(111) plt.imshow(image, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) #plt.suptitle(&quot;My Image&quot;) . #cv2.imshow(&quot;Image&quot;, image) #cv2.waitKey(0) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f1e5a6fb850&gt; . 2. Accesing Individual Pixels . # OpenCV stores images in BGR order rather than RGB (B, G, R) = image[100, 50] print(&quot;R={}, G={}, B={}&quot;.format(R, G, B)) . R=119, G=118, B=116 . 3. Array slicing/cropping . it is useful when extracting ROI (Region of Interest) | . # input image starting at x=320,y=60 at ending at x=420,y=160 roi = image[200:600, 0:500] plt.imshow(roi)#cv2.imshow(&quot;ROI&quot;, roi)cv2.waitKey(0) . &lt;matplotlib.image.AxesImage at 0x7f1e38247ad0&gt; . 4. Resizing an image . In the case of deep learning, we often resize images, ignoring aspect ratio, so that the volume fits into a network which requires that an image be square and of a certain dimension. | . resized = cv2.resize(image, (300, 300)) plt.imshow(resized) . &lt;matplotlib.image.AxesImage at 0x7f1e381860d0&gt; . #and calculate new height based on aspect-ratio in original image. r = 300.0 / w # ratio of old width /new width dim = (300, int(h * r)) resized = cv2.resize(image, dim) plt.imshow(resized) . &lt;matplotlib.image.AxesImage at 0x7f1e32db2710&gt; . 5. Rotating an image . Check imutils by pyimagesearch if you want to avoid croping the edges | . center = (w // 2, h // 2) #Rotating an image about the center point requires that we first calculate the center (x, y)-coordinates of the image #//to perform integer math #rotate image 45 deg clockwise M = cv2.getRotationMatrix2D(center, 45, 1.0) #we warp the image using the matrix (effectively rotating it) rotated = cv2.warpAffine(image, M, (w, h)) plt.imshow(rotated) #cv2.imshow(&quot;OpenCV Rotation&quot;, rotated) #cv2.waitKey(0) . &lt;matplotlib.image.AxesImage at 0x7f1e381a03d0&gt; . 6. Smoothing an image . Bluring an image reduces high-frequency noise, making it easier for NN algorithms to detect and understand the actual contents of the image rather than just noise that will “confuse” our algorithms. | . # larger kernel will give blurrier image blurred = cv2.GaussianBlur(image, (25, 25), 0) plt.imshow(blurred) . &lt;matplotlib.image.AxesImage at 0x7f1e3115ed90&gt; . 6. Drawing on an image . output = image.copy() # output: image to be drawn on # (x, y): top-left coordinates of rectangle # (x, y): right-bottom coordinates of rectangle cv2.rectangle(output, (200, 200), (600, 600), (255, 0, 240), 5) cv2.rectangle(output, (0, 200), (200, 400), (50, 205, 50), 5) cv2.circle(output, (1000, 700), 30, (255, 0, 0), -1) cv2.line(output, (800, 830), (800, 100), (255, 0, 0), 5) plt.imshow(output) . &lt;matplotlib.image.AxesImage at 0x7f1e30b6f810&gt; . output = image.copy() FONT_SIZE = 3 COLOR = (0, 255, 0) LINE_W = 2 LOC = (100, 100) cv2.putText(output, &quot;1992: M + B + T&quot;, LOC, cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE, COLOR, LINE_W) plt.imshow(output) . &lt;matplotlib.image.AxesImage at 0x7f1e305b6c90&gt; . 7.Converting an image to grayscale . channels = 1 | NN transform 1 channels to 3 for input in NN | . image = args[&quot;image02&quot;]#&quot;images/green.jpg&quot; image = cv2.imread(image) norm = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) . #fig = plt.figure(figsize = (10, 10)) #ax = fig.add_subplot(131) #ax.set_title(&quot;BGR&quot;) #plt.imshow(image) ##plt.axis(&quot;off&quot;) #ax = fig.add_subplot(132) #plt.imshow(norm) #plt.axis(&quot;off&quot;) #ax.set_title(&quot;RGB&quot;) ##plt.colorbar() #ax = fig.add_subplot(133) #ax.set_title(&quot;GRAY&quot;) #plt.imshow(gray) ##plt.axis(&quot;off&quot;) ##plt.colorbar() . . fig = plt.figure(figsize = (10,10)) images = (&quot;BGR&quot;, image), (&quot;RGB&quot;, norm), (&quot;GRAY&quot;, gray) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 3, i + 1) ax.set_title(name) plt.imshow(image, cmap = &quot;gray&quot;) plt.axis(&quot;off&quot;) . 8. Perform edge detection . image = args[&quot;image03&quot;] image = cv2.imread(image) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) plt.imshow(gray, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e180c8ed0&gt; . edged_d = cv2.Canny(gray, 4, 150) edged_c = cv2.Canny(gray, 100, 150) . images = (&quot;Coarse&quot;, edged_c), (&quot;Detailed&quot;, edged_d) fig = plt.figure(figsize = (10, 10)) for i , (name, image) in enumerate(images): ax = fig.add_subplot(1,2, i+1) ax.set_title(name) plt.imshow(image, cmap = &quot;gray&quot;) plt.axis(&quot;off&quot;) . 9. Thresholding a grayscale image . thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)[1] plt.imshow(thresh, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e18380b10&gt; . 10. Erosions and dilations . Errode away pixels to make smoother edges | . mask = thresh.copy() mask = cv2.erode(mask, None, iterations=5) plt.imshow(mask, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e18029150&gt; . 11.Detecting and drawing contours . to be continued | . References . https https://www.pyimagesearch.com/2018/07/19/opencv-tutorial-a-guide-to-learn-opencv/ .",
            "url": "https://simsisim.github.io/sims-pyimage/computer%20vision/image%20processing/opencv/2020/12/22/Day2-OpenCVTutorial.html",
            "relUrl": "/computer%20vision/image%20processing/opencv/2020/12/22/Day2-OpenCVTutorial.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Day 1 Face Detection with OpenCV and Deep Learning",
            "content": "# Convert jupyter notebook to python script: #!jupyter nbconvert --to script file-name.ipynb #$ python file-name.py --image01 image01.png etc # in ap.add_arrgument use : required = True + remove default path #args = vars(ap.parse_args()) . . import numpy as np import argparse import cv2 import os import matplotlib.pyplot as plt . #ap.add_argument(&#39;--name&#39;, &#39;-n&#39;, default=&#39;foo&#39;, help=&#39;foo&#39;) ap = argparse.ArgumentParser(description=&#39;Fooo&#39;) ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, default = &quot;images/face_detection05.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-p&quot;, &quot;--prototxt&quot;, default = &quot;models/deploy.prototxt.txt&quot;, help=&quot;path to Caffe &#39;deploy&#39; prototxt file&quot;) ap.add_argument(&quot;-m&quot;, &quot;--model&quot;, default = &quot;models/res10_300x300_ssd_iter_140000.caffemodel&quot;,#required=True, help=&quot;path to Caffe pre-trained model&quot;) ap.add_argument(&quot;-c&quot;, &quot;--confidence&quot;, type=float, default=0.90, help=&quot;minimum probability to filter weak detections&quot;) args = vars(ap.parse_args([])) #for *.py use args = vars(ap.parse_args()) . # by resizing to a fixed 300x300 pixels and then normalizing it image = cv2.imread(args[&quot;image&quot;]) (h, w) = image.shape[:2] blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0)) print(h, w) . 870 1315 . print(&quot;[INFO] loading model...&quot;) net = cv2.dnn.readNetFromCaffe(args[&quot;prototxt&quot;], args[&quot;model&quot;]) print(&quot;...done&quot;) . [INFO] loading model... ...done . # predictions print(&quot;[INFO] computing object detections...&quot;) net.setInput(blob) detections = net.forward() print(&quot;...done&quot;) . [INFO] computing object detections... ...done . for i in range(0, detections.shape[2]): # extract the confidence (i.e., probability) associated with the # prediction confidence = detections[0, 0, i, 2] # filter out weak detections by ensuring the `confidence` is # greater than the minimum confidence if confidence &gt; args[&quot;confidence&quot;]: # compute the (x, y)-coordinates of the bounding box for the # object box = detections[0, 0, i, 3:7] * np.array([w, h, w, h]) (startX, startY, endX, endY) = box.astype(&quot;int&quot;) # draw the bounding box of the face along with the associated # probability text = &quot;{:.2f}%&quot;.format(confidence * 100) y = startY - 10 if startY - 10 &gt; 10 else startY + 10 cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2) cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2) # show the output image plt.imshow(image) cv2.imwrite(&quot;images/face_detection05_t09.jpg&quot;, image) #cv2.waitKey(0) . True . fig = plt.figure(figsize = (10,10)) image_th02 = &quot;images/face_detection05_t02.jpg&quot; #image_th05 = &quot;face_detection05_t05.jpg&quot; image_th09 = &quot;images/face_detection05_t09.jpg&quot; image_th02 = cv2.imread(image_th02) #image_th05 = &quot;face_detection05_t05.jpg&quot; image_th09 = cv2.imread(image_th09) #print(image) images = (&quot;th = 0.2&quot;, image_th02), (&quot;th = 0.9&quot;, image_th09) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 2, i + 1) ax.set_title(name) plt.imshow(image) plt.axis(&quot;off&quot;) . References: . https https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/ . https https://realpython.com/command-line-interfaces-python-argparse/ . https https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc . https https://www.pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/ .",
            "url": "https://simsisim.github.io/sims-pyimage/deep%20learning/computer%20vision/2020/12/22/Day1_Face_Detection_OpenCV.html",
            "relUrl": "/deep%20learning/computer%20vision/2020/12/22/Day1_Face_Detection_OpenCV.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://simsisim.github.io/sims-pyimage/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://simsisim.github.io/sims-pyimage/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simsisim.github.io/sims-pyimage/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simsisim.github.io/sims-pyimage/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}