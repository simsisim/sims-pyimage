{
  
    
        "post0": {
            "title": "Keras Cyclical Learning Rate",
            "content": "import tensorflow as tf from tensorflow.keras.datasets import mnist import cv2 import argparse import numpy as np #from resnet import ResNet #from callbacks.epochcheckpoint import EpochCheckpoint #from callbacks.trainingmonitor import TrainingMonitor import os import sklearn import keras.backend as K from sklearn.preprocessing import LabelBinarizer from pysim.my_resnet import my_ResNet from pysim import config from pysim.clr_callback import CyclicLR import matplotlib.pyplot as plt . 1. Probleme mit herk&#246;mmlichen Lernraten . Wir wissen nicht was das optimale anfängliche Rate des Modelles ist | Eine monotone abnehmende Lernrate kann dazu führen dass unser Modell in Plateaus der Verlustsfunktion-Landschaft &quot;stecken bleibt&quot; | . Stattdessen können wir zyklische Lernraten verwenden, um unsere Lernrate zwischen oberen und unteren Grenzen zu oszillieren, was uns Folgendes ermöglicht: . Mehr Freiheit bei der Wahl der anfänglichen Lernrate | Aus Sattelpunkten und lokalen Minima auszubrechen | . 2. Die zyklische Lernrate . Die zyklische Lernrate verfolgen einen anderen Ansatz. Sie ist durch drei Paramter gekennzeichnet: . Eine minimale Lernrate definieren | Eine maximale Lernrate definieren | Lernrate soll zyklisch zwischen den beiden Grenzen oszillieren | . In der Praxis führt die Anwendung zyklischer Lernraten zu schnellerer Konvergenz, und das wird erzielt mit weniger Experimenten/Hyperparameter-Updates. Wenn man eine zyklische Lernrate im Kombination mit dem initial learning rate Ansatz verwenden führt dies zu einer Situation, in der fürs Modell nur sehr wening Feinsteurung den Parametern notwendig sei. . . 3. Die &quot;dreieckige&quot; Politik . 4. Die &quot;dreieckige2&quot; Politik . 5. Die &quot;exp_range&quot; Politik . Um die Implementierung verwenden zu können müssen wir erst einige Parameter defineren: . Batch size: Anzahl der Trainings-Beispiele, die in einem einzigen Vorwärts- und Rückwärtsdurchlauf des Modelles während des Trainings verwendet werden sollen | Batch/Iteration: Number of weight updates per epoch (i.e., # of total training examples divided by the batch size). | Cycle: Number of iterations it takes for our learning rate to go from the lower bound, ascend to the upper bound, and then descend back to the lower bound again. | Step size: Number of iterations in a half cycle. Leslie Smith, the creator of CLRs, recommends that the step_size should be (2-8) * training_iterations_in_epoch). In practice, a step sizes of either 4 or 8 work well in most situations. . NUM_CLR_CYCLES = NUM_EPOCHS / STEP_SIZE / 2 . | . (trainX, trainy), (testX, testy) = mnist.load_data() print(trainX.shape, trainy.shape, testX.shape, testy.shape) trainX, trainy, testX, testy = trainX[:100, :, :], trainy[:100], testX[:100, :, :], testy[:100,] # resize images to dimension required by model trainX = np.array([cv2.resize(image, (32, 32)) for image in trainX]) testX = np.array([cv2.resize(image, (32, 32)) for image in testX]) # scale images between 0-1 floats trainX = trainX.astype(&quot;float32&quot;)/255. testX = testX.astype(&quot;float32&quot;)/255. # reshape (batch, size1, size2, 1) trainX = np.reshape(trainX, (len(trainX), 32, 32, 1)) testX = np.reshape(trainX, (len(trainX), 32, 32, 1)) print(trainX.shape, trainy.shape, testX.shape, testy.shape) # binarizer of y-label lb = LabelBinarizer() trainy = lb.fit_transform(trainy) testy = lb.transform(testy) . (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) (100, 32, 32, 1) (100,) (100, 32, 32, 1) (100,) . model = my_ResNet(10) optimizer = tf.keras.optimizers.SGD(lr = 0.001) loss = tf.keras.losses.BinaryCrossentropy() model.compile(optimizer = optimizer, loss = loss, metrics = [&quot;accuracy&quot;]) . print(&quot;[INFO] using &#39;{}&#39; method&quot;.format(config.CLR_METHOD)) clr = CyclicLR( mode=config.CLR_METHOD, base_lr=config.MIN_LR, max_lr=config.MAX_LR,step_size= config.STEP_SIZE * (trainX.shape[0] // config.BATCH_SIZE)) . [INFO] using &#39;triangular&#39; method . H = model.fit(trainX, trainy, batch_size = config.BATCH_SIZE, validation_data = (testX, testy), steps_per_epoch = trainX.shape[0] / config.BATCH_SIZE, epochs = config.NUM_EPOCHS, callbacks = [clr], verbose = 1) . Epoch 1/96 2/1 [======================================] - 0s 176ms/step - loss: 0.3111 - accuracy: 0.4800 - val_loss: 0.3462 - val_accuracy: 0.0200 Epoch 2/96 2/1 [======================================] - 1s 277ms/step - loss: 0.3111 - accuracy: 0.4500 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 3/96 2/1 [======================================] - 0s 182ms/step - loss: 0.3114 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 4/96 2/1 [======================================] - 0s 165ms/step - loss: 0.3103 - accuracy: 0.5000 - val_loss: 0.3462 - val_accuracy: 0.0200 Epoch 5/96 2/1 [======================================] - 0s 218ms/step - loss: 0.3093 - accuracy: 0.4500 - val_loss: 0.3462 - val_accuracy: 0.0200 Epoch 6/96 2/1 [======================================] - 0s 248ms/step - loss: 0.3088 - accuracy: 0.4600 - val_loss: 0.3462 - val_accuracy: 0.0200 Epoch 7/96 2/1 [======================================] - 0s 210ms/step - loss: 0.3083 - accuracy: 0.4500 - val_loss: 0.3462 - val_accuracy: 0.0200 Epoch 8/96 2/1 [======================================] - 0s 187ms/step - loss: 0.3078 - accuracy: 0.4700 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 9/96 2/1 [======================================] - 0s 185ms/step - loss: 0.3072 - accuracy: 0.4800 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 10/96 2/1 [======================================] - 0s 218ms/step - loss: 0.3079 - accuracy: 0.4700 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 11/96 2/1 [======================================] - 0s 179ms/step - loss: 0.3073 - accuracy: 0.4700 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 12/96 2/1 [======================================] - 0s 187ms/step - loss: 0.3071 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 13/96 2/1 [======================================] - 0s 188ms/step - loss: 0.3055 - accuracy: 0.4800 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 14/96 2/1 [======================================] - 0s 194ms/step - loss: 0.3049 - accuracy: 0.4700 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 15/96 2/1 [======================================] - 0s 211ms/step - loss: 0.3049 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 16/96 2/1 [======================================] - 0s 200ms/step - loss: 0.3043 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 17/96 2/1 [======================================] - 0s 188ms/step - loss: 0.3038 - accuracy: 0.4900 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 18/96 2/1 [======================================] - 0s 211ms/step - loss: 0.3037 - accuracy: 0.4900 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 19/96 2/1 [======================================] - 0s 244ms/step - loss: 0.3033 - accuracy: 0.4800 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 20/96 2/1 [======================================] - 0s 201ms/step - loss: 0.3034 - accuracy: 0.4500 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 21/96 2/1 [======================================] - 0s 195ms/step - loss: 0.3018 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 22/96 2/1 [======================================] - 0s 209ms/step - loss: 0.3008 - accuracy: 0.4800 - val_loss: 0.3463 - val_accuracy: 0.0200 Epoch 23/96 2/1 [======================================] - 0s 183ms/step - loss: 0.3006 - accuracy: 0.4900 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 24/96 2/1 [======================================] - 0s 196ms/step - loss: 0.2998 - accuracy: 0.4900 - val_loss: 0.3463 - val_accuracy: 0.0300 Epoch 25/96 2/1 [======================================] - 0s 190ms/step - loss: 0.3002 - accuracy: 0.4800 - val_loss: 0.3463 - val_accuracy: 0.0300 Epoch 26/96 2/1 [======================================] - 0s 179ms/step - loss: 0.2998 - accuracy: 0.5100 - val_loss: 0.3464 - val_accuracy: 0.0200 Epoch 27/96 2/1 [======================================] - 0s 185ms/step - loss: 0.3030 - accuracy: 0.4700 - val_loss: 0.3464 - val_accuracy: 0.0300 Epoch 28/96 2/1 [======================================] - 0s 189ms/step - loss: 0.2998 - accuracy: 0.4800 - val_loss: 0.3463 - val_accuracy: 0.0300 Epoch 29/96 2/1 [======================================] - 0s 193ms/step - loss: 0.2994 - accuracy: 0.4800 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 30/96 2/1 [======================================] - 0s 207ms/step - loss: 0.2980 - accuracy: 0.4800 - val_loss: 0.3461 - val_accuracy: 0.0300 Epoch 31/96 2/1 [======================================] - 0s 195ms/step - loss: 0.2971 - accuracy: 0.5000 - val_loss: 0.3461 - val_accuracy: 0.0300 Epoch 32/96 2/1 [======================================] - 0s 196ms/step - loss: 0.2965 - accuracy: 0.5100 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 33/96 2/1 [======================================] - 0s 208ms/step - loss: 0.2961 - accuracy: 0.5000 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 34/96 2/1 [======================================] - 1s 252ms/step - loss: 0.2966 - accuracy: 0.5000 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 35/96 2/1 [======================================] - 0s 180ms/step - loss: 0.2959 - accuracy: 0.5100 - val_loss: 0.3462 - val_accuracy: 0.0300 Epoch 36/96 2/1 [======================================] - 0s 232ms/step - loss: 0.2958 - accuracy: 0.5100 - val_loss: 0.3461 - val_accuracy: 0.0300 Epoch 37/96 2/1 [======================================] - 0s 179ms/step - loss: 0.2953 - accuracy: 0.5000 - val_loss: 0.3460 - val_accuracy: 0.0500 Epoch 38/96 2/1 [======================================] - 0s 193ms/step - loss: 0.2950 - accuracy: 0.4800 - val_loss: 0.3460 - val_accuracy: 0.0500 Epoch 39/96 2/1 [======================================] - 0s 181ms/step - loss: 0.2946 - accuracy: 0.4900 - val_loss: 0.3460 - val_accuracy: 0.0500 Epoch 40/96 2/1 [======================================] - 0s 205ms/step - loss: 0.2936 - accuracy: 0.5100 - val_loss: 0.3460 - val_accuracy: 0.0500 Epoch 41/96 2/1 [======================================] - 0s 194ms/step - loss: 0.2935 - accuracy: 0.5000 - val_loss: 0.3461 - val_accuracy: 0.0500 Epoch 42/96 2/1 [======================================] - 0s 231ms/step - loss: 0.2944 - accuracy: 0.5100 - val_loss: 0.3461 - val_accuracy: 0.0500 Epoch 43/96 2/1 [======================================] - 0s 219ms/step - loss: 0.2928 - accuracy: 0.5100 - val_loss: 0.3461 - val_accuracy: 0.0500 Epoch 44/96 2/1 [======================================] - 0s 167ms/step - loss: 0.2930 - accuracy: 0.5100 - val_loss: 0.3459 - val_accuracy: 0.0500 Epoch 45/96 2/1 [======================================] - 0s 171ms/step - loss: 0.2921 - accuracy: 0.5100 - val_loss: 0.3459 - val_accuracy: 0.0500 Epoch 46/96 2/1 [======================================] - 0s 169ms/step - loss: 0.2922 - accuracy: 0.5200 - val_loss: 0.3458 - val_accuracy: 0.0500 Epoch 47/96 2/1 [======================================] - 0s 174ms/step - loss: 0.2909 - accuracy: 0.5200 - val_loss: 0.3458 - val_accuracy: 0.0500 Epoch 48/96 2/1 [======================================] - 1s 272ms/step - loss: 0.2902 - accuracy: 0.5200 - val_loss: 0.3458 - val_accuracy: 0.0500 Epoch 49/96 2/1 [======================================] - 1s 264ms/step - loss: 0.2905 - accuracy: 0.5200 - val_loss: 0.3458 - val_accuracy: 0.0500 Epoch 50/96 2/1 [======================================] - 1s 263ms/step - loss: 0.2905 - accuracy: 0.5000 - val_loss: 0.3459 - val_accuracy: 0.0500 Epoch 51/96 2/1 [======================================] - 0s 202ms/step - loss: 0.2905 - accuracy: 0.5000 - val_loss: 0.3459 - val_accuracy: 0.0500 Epoch 52/96 2/1 [======================================] - 0s 158ms/step - loss: 0.2903 - accuracy: 0.5100 - val_loss: 0.3458 - val_accuracy: 0.0600 Epoch 53/96 2/1 [======================================] - 0s 162ms/step - loss: 0.2897 - accuracy: 0.5100 - val_loss: 0.3457 - val_accuracy: 0.0700 Epoch 54/96 2/1 [======================================] - 0s 153ms/step - loss: 0.2883 - accuracy: 0.5000 - val_loss: 0.3456 - val_accuracy: 0.0700 Epoch 55/96 2/1 [======================================] - 0s 154ms/step - loss: 0.2877 - accuracy: 0.5200 - val_loss: 0.3456 - val_accuracy: 0.0700 Epoch 56/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2880 - accuracy: 0.5000 - val_loss: 0.3456 - val_accuracy: 0.0800 Epoch 57/96 2/1 [======================================] - 0s 153ms/step - loss: 0.2876 - accuracy: 0.5300 - val_loss: 0.3457 - val_accuracy: 0.0800 Epoch 58/96 2/1 [======================================] - 0s 151ms/step - loss: 0.2870 - accuracy: 0.5300 - val_loss: 0.3457 - val_accuracy: 0.0800 Epoch 59/96 2/1 [======================================] - 0s 153ms/step - loss: 0.2871 - accuracy: 0.5000 - val_loss: 0.3456 - val_accuracy: 0.0800 Epoch 60/96 2/1 [======================================] - 0s 247ms/step - loss: 0.2857 - accuracy: 0.5300 - val_loss: 0.3455 - val_accuracy: 0.0800 Epoch 61/96 2/1 [======================================] - 1s 272ms/step - loss: 0.2876 - accuracy: 0.5100 - val_loss: 0.3454 - val_accuracy: 0.0800 Epoch 62/96 2/1 [======================================] - 1s 250ms/step - loss: 0.2861 - accuracy: 0.5300 - val_loss: 0.3453 - val_accuracy: 0.0800 Epoch 63/96 2/1 [======================================] - 0s 166ms/step - loss: 0.2844 - accuracy: 0.5200 - val_loss: 0.3453 - val_accuracy: 0.0700 Epoch 64/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2841 - accuracy: 0.5200 - val_loss: 0.3453 - val_accuracy: 0.0700 Epoch 65/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2848 - accuracy: 0.5300 - val_loss: 0.3453 - val_accuracy: 0.0700 Epoch 66/96 2/1 [======================================] - 0s 159ms/step - loss: 0.2833 - accuracy: 0.5400 - val_loss: 0.3454 - val_accuracy: 0.0700 Epoch 67/96 2/1 [======================================] - 0s 205ms/step - loss: 0.2843 - accuracy: 0.5100 - val_loss: 0.3453 - val_accuracy: 0.0800 Epoch 68/96 2/1 [======================================] - 0s 204ms/step - loss: 0.2843 - accuracy: 0.5300 - val_loss: 0.3452 - val_accuracy: 0.0700 Epoch 69/96 2/1 [======================================] - 0s 170ms/step - loss: 0.2833 - accuracy: 0.5300 - val_loss: 0.3451 - val_accuracy: 0.0800 Epoch 70/96 2/1 [======================================] - 0s 167ms/step - loss: 0.2823 - accuracy: 0.5400 - val_loss: 0.3451 - val_accuracy: 0.0800 Epoch 71/96 2/1 [======================================] - 0s 165ms/step - loss: 0.2824 - accuracy: 0.4900 - val_loss: 0.3451 - val_accuracy: 0.0800 Epoch 72/96 2/1 [======================================] - 0s 157ms/step - loss: 0.2829 - accuracy: 0.5000 - val_loss: 0.3451 - val_accuracy: 0.0900 Epoch 73/96 2/1 [======================================] - 0s 164ms/step - loss: 0.2813 - accuracy: 0.5300 - val_loss: 0.3451 - val_accuracy: 0.0900 Epoch 74/96 2/1 [======================================] - 0s 158ms/step - loss: 0.2816 - accuracy: 0.5100 - val_loss: 0.3451 - val_accuracy: 0.1000 Epoch 75/96 2/1 [======================================] - 0s 203ms/step - loss: 0.2809 - accuracy: 0.5500 - val_loss: 0.3451 - val_accuracy: 0.1100 Epoch 76/96 2/1 [======================================] - 0s 168ms/step - loss: 0.2803 - accuracy: 0.5400 - val_loss: 0.3450 - val_accuracy: 0.1200 Epoch 77/96 2/1 [======================================] - 0s 149ms/step - loss: 0.2797 - accuracy: 0.5200 - val_loss: 0.3450 - val_accuracy: 0.1200 Epoch 78/96 2/1 [======================================] - 0s 152ms/step - loss: 0.2805 - accuracy: 0.5200 - val_loss: 0.3449 - val_accuracy: 0.1200 Epoch 79/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2813 - accuracy: 0.5300 - val_loss: 0.3448 - val_accuracy: 0.1200 Epoch 80/96 2/1 [======================================] - 0s 190ms/step - loss: 0.2787 - accuracy: 0.5200 - val_loss: 0.3448 - val_accuracy: 0.1200 Epoch 81/96 2/1 [======================================] - 0s 171ms/step - loss: 0.2790 - accuracy: 0.5500 - val_loss: 0.3449 - val_accuracy: 0.1200 Epoch 82/96 2/1 [======================================] - 0s 152ms/step - loss: 0.2798 - accuracy: 0.5200 - val_loss: 0.3449 - val_accuracy: 0.1200 Epoch 83/96 2/1 [======================================] - 0s 150ms/step - loss: 0.2779 - accuracy: 0.5200 - val_loss: 0.3448 - val_accuracy: 0.1200 Epoch 84/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2772 - accuracy: 0.5400 - val_loss: 0.3449 - val_accuracy: 0.1200 Epoch 85/96 2/1 [======================================] - 0s 159ms/step - loss: 0.2780 - accuracy: 0.5400 - val_loss: 0.3448 - val_accuracy: 0.1100 Epoch 86/96 2/1 [======================================] - 0s 150ms/step - loss: 0.2773 - accuracy: 0.5200 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 87/96 2/1 [======================================] - 0s 152ms/step - loss: 0.2760 - accuracy: 0.5300 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 88/96 2/1 [======================================] - 0s 160ms/step - loss: 0.2751 - accuracy: 0.5300 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 89/96 2/1 [======================================] - 0s 191ms/step - loss: 0.2755 - accuracy: 0.5700 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 90/96 2/1 [======================================] - 0s 162ms/step - loss: 0.2769 - accuracy: 0.5400 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 91/96 2/1 [======================================] - 0s 163ms/step - loss: 0.2758 - accuracy: 0.5200 - val_loss: 0.3447 - val_accuracy: 0.1100 Epoch 92/96 2/1 [======================================] - 0s 153ms/step - loss: 0.2751 - accuracy: 0.5300 - val_loss: 0.3446 - val_accuracy: 0.1100 Epoch 93/96 2/1 [======================================] - 0s 179ms/step - loss: 0.2741 - accuracy: 0.5400 - val_loss: 0.3446 - val_accuracy: 0.1100 Epoch 94/96 2/1 [======================================] - 0s 155ms/step - loss: 0.2744 - accuracy: 0.5000 - val_loss: 0.3445 - val_accuracy: 0.1100 Epoch 95/96 2/1 [======================================] - 0s 205ms/step - loss: 0.2739 - accuracy: 0.5300 - val_loss: 0.3445 - val_accuracy: 0.1100 Epoch 96/96 2/1 [======================================] - 0s 149ms/step - loss: 0.2726 - accuracy: 0.5400 - val_loss: 0.3445 - val_accuracy: 0.1100 . print(&quot;[INFO] evaluating network...&quot;) predictions = model.predict(x=testX, batch_size=config.BATCH_SIZE) #print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=config.CLASSES)) . [INFO] evaluating network... . N = np.arange(0, config.NUM_EPOCHS) plt.style.use(&quot;ggplot&quot;) plt.figure() plt.plot(N, H.history[&quot;loss&quot;], label=&quot;train_loss&quot;) plt.plot(N, H.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;) plt.plot(N, H.history[&quot;accuracy&quot;], label=&quot;train_acc&quot;) plt.plot(N, H.history[&quot;val_accuracy&quot;], label=&quot;val_acc&quot;) plt.title(&quot;Training Loss and Accuracy&quot;) plt.xlabel(&quot;Epoch #&quot;) plt.ylabel(&quot;Loss/Accuracy&quot;) plt.legend(loc=&quot;lower left&quot;) plt.savefig(config.TRAINING_PLOT_PATH) . N = np.arange(0, len(clr.history[&quot;lr&quot;])) plt.figure() plt.plot(N, clr.history[&quot;lr&quot;]) plt.title(&quot;Cyclical Learning Rate (CLR)&quot;) plt.xlabel(&quot;Training Iterations&quot;) plt.ylabel(&quot;Learning Rate&quot;) plt.savefig(config.CLR_PLOT_PATH) . References . Adrian Rosebrock, OpenCV Face Recognition, PyImageSearch, https://www.pyimagesearch.com/, accessed on 3 January, 2021&gt; www:https://www.pyimagesearch.com/2019/10/07/is-rectified-adam-actually-better-than-adam/ .",
            "url": "https://simsisim.github.io/sims-pyimage/keras/2021/01/12/cyclical_learning_rate.html",
            "relUrl": "/keras/2021/01/12/cyclical_learning_rate.html",
            "date": " • Jan 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Learning Rate Finder",
            "content": "1. Automatical learning rate finder . Step 1: We start by defining an upper and lower bound on our learning rate. The lower bound should be very small (1e-10) and the upper bound should be very large (1e+1). . convergence (low lr) - divergence (high lr) | . Step 2: We then start training our network, starting at the lower bound. . after each batch we increase the learning rate --&gt; exponentially increase | after each batch register/save learning rate and loss for each batch | . Step 3: Training continues, and therefore the learning rate continues to increase until we hit our maximum learning rate value. . typically, this entire training process/learning rate increase only takes 1-5 epochs | . Step 4: After training is complete we plot a smoothed loss over time, enabling us to see when the learning rate is both. . Just large enough for loss to decrease . | And too large, to the point where loss starts to increase . | . . . 2. Initialize NN of learning rate using obtained lower-upper bounds . Cyclical Learning Rate policy - use both lower-upper bounds . | Learning Rate Scheduele/Decay policy - use just the upper bound and decrease the learning rate as training progresses . | . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import pandas as pd import keras K = keras.backend from sklearn.datasets import load_breast_cancer from sklearn.preprocessing import StandardScaler . data = load_breast_cancer() X_train, y_train = data.data, data.target scaler = StandardScaler() X_train = scaler.fit_transform(X_train) . input_ = tf.keras.layers.Input(shape = (X_train.shape[1],)) hidden1 = tf.keras.layers.Dense(units = 10, activation = &quot;relu&quot;)(input_) hidden2 = tf.keras.layers.Dense(units = 10, activation = &quot;relu&quot;)(hidden1) output = tf.keras.layers.Dense(units = 2, activation = &quot;sigmoid&quot;)(hidden2) model = tf.keras.Model(inputs = [input_], outputs = [output]) optimizer = tf.keras.optimizers.SGD() model.compile(loss = tf.keras.losses.MeanSquaredError(), optimizer = optimizer) . class showLR(keras.callbacks.Callback) : def on_batch_begin(self, batch, logs=None): lr = float(K.get_value(self.model.optimizer.lr)) print (&quot; batch={:02d}, lr={:.5f}&quot;.format( batch, lr )) return lr class ExponentialLearningRate(keras.callbacks.Callback): def __init__(self, factor): self.factor = factor self.rates = [] self.losses = [] def on_batch_end(self, batch, logs): self.rates.append(K.get_value(self.model.optimizer.lr)) self.losses.append(logs[&quot;loss&quot;]) K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor) . def learning_rate_finder(model, X_train, y_train, epochs, batch_size, min_rate, max_rate): # get weights that were used to initialize model init_weights = model.get_weights() # get and save initial leraning rate of model init_lr = K.get_value(model.optimizer.lr) # iterations = steps_per_epoch iterations = epochs * len(X_train)/(batch_size) # steps_per_epoch # factor for computing expoenetial growth factor = np.exp(np.log(max_rate / min_rate) / iterations) # at batch = 0 set the learning rate at min_rate K.set_value(model.optimizer.lr, min_rate) # at each computed batch = 1,2,3, ... increase the learning rate by exponential growth exp_lr = ExponentialLearningRate(factor) # fit model history = model.fit(X_train, y_train, epochs=1, batch_size = batch_size, callbacks=[exp_lr, showLR()]) return exp_lr.rates, exp_lr.losses rates, losses = learning_rate_finder(model, X_train, y_train, epochs=1, batch_size=12, min_rate=0.05, max_rate = 100) . batch=00, lr=0.05000 1/48 [..............................] - ETA: 0s - loss: 0.1667 batch=01, lr=0.05869 batch=02, lr=0.06890 batch=03, lr=0.08088 batch=04, lr=0.09494 batch=05, lr=0.11144 batch=06, lr=0.13082 batch=07, lr=0.15357 batch=08, lr=0.18026 batch=09, lr=0.21161 batch=10, lr=0.24840 batch=11, lr=0.29158 batch=12, lr=0.34228 batch=13, lr=0.40179 batch=14, lr=0.47165 batch=15, lr=0.55365 batch=16, lr=0.64991 17/48 [=========&gt;....................] - ETA: 0s - loss: 0.2132 batch=17, lr=0.76290 batch=18, lr=0.89554 batch=19, lr=1.05124 batch=20, lr=1.23401 batch=21, lr=1.44856 batch=22, lr=1.70042 batch=23, lr=1.99606 batch=24, lr=2.34310 batch=25, lr=2.75048 batch=26, lr=3.22868 batch=27, lr=3.79003 batch=28, lr=4.44898 batch=29, lr=5.22250 batch=30, lr=6.13050 batch=31, lr=7.19637 batch=32, lr=8.44755 33/48 [===================&gt;..........] - ETA: 0s - loss: 0.2462 batch=33, lr=9.91627 batch=34, lr=11.64034 batch=35, lr=13.66417 batch=36, lr=16.03987 batch=37, lr=18.82861 batch=38, lr=22.10222 batch=39, lr=25.94498 batch=40, lr=30.45586 batch=41, lr=35.75102 batch=42, lr=41.96681 batch=43, lr=49.26329 batch=44, lr=57.82837 batch=45, lr=67.88261 batch=46, lr=79.68490 batch=47, lr=93.53918 48/48 [==============================] - 0s 3ms/step - loss: 0.2487 . def plot_lr_vs_loss(rates, losses): plt.plot(rates, losses) plt.gca().set_xscale(&#39;log&#39;) plt.hlines(min(losses), min(rates), max(rates)) plt.axis([min(rates), max(rates), min(losses), max(losses)]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;Loss&quot;) plt.show() plot_lr_vs_loss(rates, losses) . References . Adrian Rosebrock, OpenCV Face Recognition, PyImageSearch, https://www.pyimagesearch.com/, accessed on 3, January, 2021&gt; https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/ .",
            "url": "https://simsisim.github.io/sims-pyimage/keras/2021/01/04/learning_rate_finder.html",
            "relUrl": "/keras/2021/01/04/learning_rate_finder.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "OpenCV Selective Search",
            "content": "import tensorflow as tf import numpy as np import argparse import cv2 print(tf.__version__) print(cv2.__version__) import matplotlib.pyplot as plt import time . 2.3.0 4.4.0 . 1. Selective Search for Objection Recognition (2012) . Replaces sliding window and hierachical pyramids technique. The tenchique does not perfoms classification by itself (at needs a classifier for that) . | Selective Search is far more computationally efficient than exhaustively computing image pyramids and sliding windows (very slow + additional parameters to be tuned) . | . Image can be over-segmented to automatically identify locations in an image that could contain an object . | Paved the way for RPN (Region Proposal Network) &amp; R-CNN . | . 2. Algorithm steps . Accepts an image as input | Over-segment it by applying a superpixel clustering algorithm | Merge segments of the superpixels based on five components: Color similarity | Texture similarity | Size similarity | Shape similarity | A final meta-similarity, which is a linear combination of the above similarity measures | . | 3. Region proposal algorithm needs . Be faster and more efficient than sliding windows and image pyramids | Accurately detect the regions of an image that could contain an object | Pass these “candidate proposals” to a downstream classifier to actually label the regions, thus completing the object detection framework | . ap = argparse.ArgumentParser() ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, default = &quot;images/keras_detection/dlui03.jpg&quot;, required = False, help=&quot;path to the input image&quot;) ap.add_argument(&quot;-m&quot;, &quot;--method&quot;, type=str, default=&quot;fast&quot;, choices=[&quot;fast&quot;, &quot;quality&quot;], help=&quot;selective search method&quot;) args = vars(ap.parse_args([])) . image = cv2.imread(args[&quot;image&quot;]) image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) w = 255 scale = w/(image.shape[1]) h = int(scale * image.shape[0]) image = cv2.resize(image, (w, h)) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f619e028cd0&gt; . # input image ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation() ss.setBaseImage(image) # check to see if we are using the *fast* but *less accurate* version # of selective search if args[&quot;method&quot;] == &quot;fast&quot;: print(&quot;[INFO] using *fast* selective search&quot;) ss.switchToSelectiveSearchFast() # otherwise we are using the *slower* but *more accurate* version else: print(&quot;[INFO] using *quality* selective search&quot;) ss.switchToSelectiveSearchQuality() # run selective search on the input image . [INFO] using *fast* selective search . start = time.time() rects = ss.process() end = time.time() # show how along selective search took to run along with the total # number of returned region proposals print(&quot;[INFO] selective search took {:.4f} seconds&quot;.format(end - start)) print(&quot;[INFO] {} total region proposals&quot;.format(len(rects))) # loop over the region proposals in chunks (so we can better # visualize them) for i in range(0, len(rects), 100): # clone the original image so we can draw on it output = image.copy() # loop over the current subset of region proposals for (x, y, w, h) in rects[i:i + 100]: # draw the region proposal bounding box on the image color = [np.random.randint(0, 255) for j in range(0, 3)] cv2.rectangle(output, (x, y), (x + w, y + h), color, 2) # show the output image #cv2.imshow(&quot;Output&quot;, output) plt.imshow(output) #cv2.imshow(&quot;Output&quot;, output) #key = cv2.waitKey(0) &amp; 0xFF # if the `q` key was pressed, break from the loop #if key == ord(&quot;q&quot;): # break . [INFO] selective search took 0.8691 seconds [INFO] 707 total region proposals . References . References:https://www.pyimagesearch.com/2020/06/29/opencv-selective-search-for-object-detection/ . .",
            "url": "https://simsisim.github.io/sims-pyimage/image%20processing/opencv/2020/12/27/selectiveSearchOpenCV.html",
            "relUrl": "/image%20processing/opencv/2020/12/27/selectiveSearchOpenCV.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Object Detection using Region Proposal Algorithm",
            "content": "import tensorflow as tf import numpy as np import argparse import cv2 print(tf.__version__) print(cv2.__version__) import matplotlib.pyplot as plt import time from imutils.object_detection import non_max_suppression . 2.3.0 4.4.0 . 1. HGFHDGFHG . RPA generates in an image regions based on superpixels custering algorithms. . ap = argparse.ArgumentParser() ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, default = &quot;images/hens.jpg&quot;, required = False, help=&quot;path to the input image&quot;) ap.add_argument(&quot;-m&quot;, &quot;--method&quot;, type=str, default=&quot;fast&quot;, choices=[&quot;fast&quot;, &quot;quality&quot;], help=&quot;selective search method&quot;) ap.add_argument(&quot;-c&quot;, &quot;--conf&quot;, type=float, default=0.7, help=&quot;minimum probability to consider a classification/detection&quot;) ap.add_argument(&quot;-f&quot;, &quot;--filter&quot;, type=str, default=None, help=&quot;comma separated list of ImageNet labels to filter on&quot;) args = vars(ap.parse_args([])) . labelFilters = args[&quot;filter&quot;] image = cv2.imread(args[&quot;image&quot;]) image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) w = 255 scale = w/(image.shape[1]) h = int(scale * image.shape[0]) image = cv2.resize(image, (w, h)) plt.imshow(image) (H, W) = image.shape[:2] . # initialize OpenCV&#39;s selective search implementation and set the # input image ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation() ss.setBaseImage(image) # check to see if we are using the *fast* but *less accurate* version # of selective search if args[&quot;method&quot;] == &quot;fast&quot;: print(&quot;[INFO] using *fast* selective search&quot;) ss.switchToSelectiveSearchFast() # otherwise we are using the *slower* but *more accurate* version else: print(&quot;[INFO] using *quality* selective search&quot;) ss.switchToSelectiveSearchQuality() # run selective search on the input image . . [INFO] using *fast* selective search . start = time.time() rects = ss.process() end = time.time() # show how along selective search took to run along with the total # number of returned region proposals print(&quot;[INFO] selective search took {:.4f} seconds&quot;.format(end - start)) print(&quot;[INFO] {} total region proposals&quot;.format(len(rects))) # loop over the region proposals in chunks (so we can better # visualize them) for i in range(0, len(rects), 100): # clone the original image so we can draw on it output = image.copy() # loop over the current subset of region proposals for (x, y, w, h) in rects[i:i + 100]: # draw the region proposal bounding box on the image color = [np.random.randint(0, 255) for j in range(0, 3)] cv2.rectangle(output, (x, y), (x + w, y + h), color, 2) # show the output image #cv2.imshow(&quot;Output&quot;, output) #plt.imshow(&quot;selectiveSearch.jpg&quot;) #cv2.imshow(&quot;Output&quot;, output) #key = cv2.waitKey(0) &amp; 0xFF # if the `q` key was pressed, break from the loop #if key == ord(&quot;q&quot;): #break . [INFO] selective search took 0.8311 seconds [INFO] 655 total region proposals . model = tf.keras.applications.ResNet50(weights = &quot;imagenet&quot;, include_top = True) . proposals = [] boxes = [] # loop over the region proposal bounding box coordinates generated by # running selective search for (x, y, w, h) in rects: # if the width or height of the region is less than 10% of the # image width or height, ignore it (i.e., filter out small # objects that are likely false-positives) if w / float(W) &lt; 0.1 or h / float(H) &lt; 0.1: continue # extract the region from the input image, convert it from BGR to # RGB channel ordering, and then resize it to 224x224 (the input # dimensions required by our pre-trained CNN) roi = image[y:y + h, x:x + w] roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB) roi = cv2.resize(roi, (224, 224)) # further preprocess by the ROI roi = tf.keras.preprocessing.image.img_to_array(roi) roi = tf.keras.applications.resnet50.preprocess_input(roi) # update our proposals and bounding boxes lists proposals.append(roi) boxes.append((x, y, w, h)) . proposals = np.array(proposals) print(&quot;[INFO] proposal shape: {}&quot;.format(proposals.shape)) print(&quot;[INFO] proposal number: {}&quot;.format(len(proposals))) # classify each of the proposal ROIs using ResNet and then decode the # predictions print(&quot;[INFO] classifying proposals...&quot;) preds = model.predict(proposals) preds = tf.keras.applications.imagenet_utils.decode_predictions(preds, top=1) # initialize a dictionary which maps class labels (keys) to any # bounding box associated with that label (values) labels = {} . [INFO] proposal shape: (397, 224, 224, 3) [INFO] proposal number: 397 [INFO] classifying proposals... . preds[15:30][0] . [(&#39;n02066245&#39;, &#39;grey_whale&#39;, 0.1794284)] . for (i, p) in enumerate(preds): # grab the prediction information for the current region proposal (imagenetID, label, prob) = p[0] # only if the label filters are not empty *and* the label does not # exist in the list, then ignore it if labelFilters is not None and label not in labelFilters: continue # filter out weak detections by ensuring the predicted probability # is greater than the minimum probability if prob &gt;= args[&quot;conf&quot;]: # grab the bounding box associated with the prediction and # convert the coordinates (x, y, w, h) = boxes[i] box = (x, y, x + w, y + h) # grab the list of predictions for the label and add the # bounding box + probability to the list L = labels.get(label, []) L.append((box, prob)) labels[label] = L . for label in labels.keys(): # clone the original image so that we can draw on it print(&quot;[INFO] showing results for &#39;{}&#39;&quot;.format(label)) clone = image.copy() # loop over all bounding boxes for the current label for (box, prob) in labels[label]: # draw the bounding box on the image (startX, startY, endX, endY) = box cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) # show the results *before* applying non-maxima suppression, then # clone the image again so we can display the results *after* # applying non-maxima suppression #cv2.imshow(&quot;Before&quot;, clone) plt.imshow(clone) plt.savefig(&quot;images/keras_detection/hens_before.jpg&quot;) clone = image.copy() # extract the bounding boxes and associated prediction # probabilities, then apply non-maxima suppression boxes = np.array([p[0] for p in labels[label]]) proba = np.array([p[1] for p in labels[label]]) boxes = non_max_suppression(boxes, proba) # loop over all bounding boxes that were kept after applying # non-maxima suppression for (startX, startY, endX, endY) in boxes: # draw the bounding box and label on the image cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) y = startY - 10 if startY - 10 &gt; 10 else startY + 10 cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2) # show the output after apply non-maxima suppression plt.imshow(clone) plt.savefig(&quot;images/keras_detection/hens_rpa_after.jpg&quot;) . [INFO] showing results for &#39;grey_whale&#39; [INFO] showing results for &#39;safety_pin&#39; [INFO] showing results for &#39;harvestman&#39; [INFO] showing results for &#39;American_coot&#39; [INFO] showing results for &#39;black_grouse&#39; [INFO] showing results for &#39;fire_screen&#39; [INFO] showing results for &#39;bluetick&#39; [INFO] showing results for &#39;paddle&#39; . allclone = image.copy() for label in labels.keys(): # clone the original image so that we can draw on it print(&quot;[INFO] showing results for &#39;{}&#39;&quot;.format(label)) clone = image.copy() # loop over all bounding boxes for the current label for (box, prob) in labels[label]: # draw the bounding box on the image (startX, startY, endX, endY) = box cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) # show the results *before* applying non-maxima suppression, then # clone the image again so we can display the results *after* # applying non-maxima suppression #plt.imshow(clone) #cv2.imshow(&quot;Before&quot;, clone) clone = image.copy() # extract the bounding boxes and associated prediction # probabilities, then apply non-maxima suppression boxes = np.array([p[0] for p in labels[label]]) proba = np.array([p[1] for p in labels[label]]) boxes = non_max_suppression(boxes, proba) # loop over all bounding boxes that were kept after applying # non-maxima suppression for (startX, startY, endX, endY) in boxes: # draw the bounding box and label on the image cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) cv2.rectangle(allclone, (startX, startY), (endX, endY),(0, 255, 0), 2) y = startY - 10 if startY - 10 &gt; 10 else startY + 10 cv2.putText(clone, label, (startX, y+30),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2) cv2.putText(clone, &quot;{:.2f}&quot;.format(prob), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2) cv2.putText(allclone, label, (startX, y+30),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2) cv2.putText(allclone, &quot;{:.2f}&quot;.format(prob), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2) # show the output after apply non-maxima suppression plt.imshow(clone) plt.imsave(&quot;images/keras_detection/_rpa&quot;+ &quot;_hens_&quot; + label + &quot;.jpg&quot;, clone) plt.imshow(allclone) plt.imsave(&quot;images/keras_detection/_rpa_allclone&quot; + &quot;_hens&quot; + &quot;.jpg&quot;, allclone) #plt.imshow(clone) #plt.imsave(&quot;images/_res03.jpg&quot;, clone) #cv2.imshow(&quot;After&quot;, clone) #cv2.waitKey(0) . [INFO] showing results for &#39;grey_whale&#39; [INFO] showing results for &#39;safety_pin&#39; [INFO] showing results for &#39;harvestman&#39; [INFO] showing results for &#39;American_coot&#39; [INFO] showing results for &#39;black_grouse&#39; [INFO] showing results for &#39;fire_screen&#39; [INFO] showing results for &#39;bluetick&#39; [INFO] showing results for &#39;paddle&#39; . References . https://www.pyimagesearch.com/2020/07/06/region-proposal-object-detection-with-opencv-keras-and-tensorflow/ .",
            "url": "https://simsisim.github.io/sims-pyimage/keras/cnn/opencv/2020/12/27/RegionProposalAlgo-ObjectDetection.html",
            "relUrl": "/keras/cnn/opencv/2020/12/27/RegionProposalAlgo-ObjectDetection.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Create gifs",
            "content": "import glob from PIL import Image import glob # filepaths fp_in = &quot;images/keras_detection/clone_p*.jpg&quot; fp_out = &quot;images/keras_detection/sliding_window_sw.gif&quot; # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif img, *imgs = [Image.open(f) for f in sorted(glob.glob(fp_in))] img.save(fp=fp_out, format=&#39;GIF&#39;, append_images=imgs, save_all=True, duration=2000, loop=0) . fp_in = &quot;images/keras_detection/roiOrig_p*.jpg&quot; fp_out = &quot;images/keras_detection/sliding_window_rois.gif&quot; # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif img, *imgs = [Image.open(f) for f in sorted(glob.glob(fp_in))] img.save(fp=fp_out, format=&#39;GIF&#39;, append_images=imgs, save_all=True, duration=2000, loop=0) . Sliding window and ROIs for an image with three hierachical pyramids . .",
            "url": "https://simsisim.github.io/sims-pyimage/image%20processing/pil/2020/12/23/Gifs_PIL.html",
            "relUrl": "/image%20processing/pil/2020/12/23/Gifs_PIL.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Turn a CNN into an object classifier",
            "content": "1. Classification vs Object Detection . Classification: . One image in | One class label out | . Object detection: . Input one image | Obtain multiple bounding boxes and class labels as output | . import imutils import numpy as np import cv2 import matplotlib.pyplot as plt import argparse import tensorflow as tf import imutils from imutils.object_detection import non_max_suppression import time print(tf.__version__, cv2.__version__) . . 2.3.0 4.4.0 . class_cat = cv2.imread(&quot;images/class_cat.jpg&quot;) class_cat = cv2.cvtColor(class_cat, cv2.COLOR_BGR2RGB) obj_cat = cv2.imread(&quot;images/obj_cat.jpg&quot;) obj_cat = cv2.cvtColor(obj_cat, cv2.COLOR_BGR2RGB) . . fig = plt.figure(figsize = (10,10)) images = (&quot;Classification&quot;, class_cat), (&quot;Object Detection&quot;, obj_cat) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 2, i + 1) ax.set_title(name) plt.imshow(image) plt.axis(&quot;off&quot;) . . 2. Object detection algorithm pattern . Input: an image that we wish to apply object detection to . | Output: has three values: . 2a. A list of bounding boxes, or the (x, y)-coordinates for each object in image . 2b. The class label associated with each of the bounding boxes . 2c. The probability/confidence score associated with each bounding box and class label . | orig_cat = cv2.imread(&quot;images/keras-detection/dlui03.jpg&quot;) orig_cat = cv2.cvtColor(orig_cat, cv2.COLOR_BGR2RGB ) class_cat = orig_cat.copy() cv2.putText(class_cat, &quot;{:.2f}&quot;.format(0.80), (100, 200),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) cv2.putText(class_cat, &quot;cat&quot;, (100, 100),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) plt.imshow(class_cat) plt.savefig(&quot;images/keras-detection/class_cat.jpg&quot;) obj_cat = orig_cat.copy() cv2.putText(obj_cat, &quot;{:.2f}&quot;.format(0.70), (100, 200),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) cv2.putText(obj_cat, &quot;cat&quot;, (100, 100),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) cv2.rectangle(obj_cat, (500,10), (950, 600), (255, 0, 0), 6) #obj_cat2 = orig_cat.copy() cv2.putText(obj_cat, &quot;{:.2f}&quot;.format(0.70), (750, 1500),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) cv2.putText(obj_cat, &quot;cat&quot;, (750, 1400),cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 0, 0), 6) cv2.rectangle(obj_cat, (250,1000), (600, 1400), (255, 0, 0), 6) plt.imshow(obj_cat) plt.savefig(&quot;images/keras-detection/obj_cat.jpg&quot;) . . fig = plt.figure(figsize = (10,10)) images = (&quot;Classification&quot;, class_cat), (&quot;Object Detection&quot;, obj_cat) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 2, i + 1) ax.set_title(name) plt.imshow(image) plt.axis(&quot;off&quot;) . . 2. Turn any classifier into an object detector . Before ANN-CNN era state of the for object detection was HOG(Histogarm of Oriented Gardients) + SVM | This tutorial combines several approaches: . Image pyramids: Localize objects at different scales/sizes (is a multi-scale representation of an image) | Sliding windows: Detect exactly where in the image a given object is. | Non-maxima suppression: Collapse weak, overlapping bounding box | | . import imutils import numpy as np import cv2 import matplotlib.pyplot as plt import argparse import tensorflow as tf import imutils from imutils.object_detection import non_max_suppression import time . ap = argparse.ArgumentParser() ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, default = &quot;images/keras-detection/dlui03.jpg&quot;, #required=False, help=&quot;path to the input image&quot;) ap.add_argument(&quot;-s&quot;, &quot;--size&quot;, type=str, default=&quot;(200, 150)&quot;, help=&quot;ROI size (in pixels)&quot;) ap.add_argument(&quot;-c&quot;, &quot;--min-conf&quot;, type=float, default=0.7, help=&quot;minimum probability to filter weak detections&quot;) ap.add_argument(&quot;-v&quot;, &quot;--visualize&quot;, type=int, default=1, help=&quot;whether or not to show extra visualizations for debugging&quot;) args = vars(ap.parse_args([])) . . image = cv2.imread(args[&quot;image&quot;]) #print(image.shape) # resize image keeping aspect ratio #r = 224 / image.shape[1] # ratio of new width /old width #dim = (224, int(image.shape[0] * r)) # resized height #image = cv2.resize(image, dim) # move to RGB map image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # display image fig = plt.figure(figsize = (5, 5)) ax = fig.add_subplot(111) ax.set_title(&quot;cat: dlui&quot;) plt.imshow(image) plt.axis(&quot;off&quot;) plt.colorbar() . . &lt;matplotlib.colorbar.Colorbar at 0x7fcd5c1b5c50&gt; . 3. Construct a sliding window generator . The sliding function is constructed as a generator. . Sliding window and ROIs for an image with three hierachical pyramids. . . . def sliding_window(image, step, ws): #slide a window of ws size over the image for y in range(0, image.shape[0]-ws[1], step): # rows-wise loop # -ws[1] avoids extending the sliding window outside the image itself, increment the y-position with step for x in range(0, image.shape[1] - ws[0], step):#columns-wise loop, increment the x-position with step # use yield(instead of return) because this is a generator #yield the actual x and y positions and the current window yield (x, y, image[y:y + ws[1], x:x + ws[0]]) . 4. Construct a pyramid generator . The image_pyramid function is constructed as a generator. . At the bottom of the pyramid, we have the original image at its original size (in terms of width and height). At each subsequent layer, the image is resized (subsampled) by a scaling factor and optionally smoothed (usually via Gaussian blurring). The image is progressively subsampled(adding pyramids) until some stopping criterion is met, which is normally when a minimum size has been reached(smaller than the sliding window size). . Image resizing take place in two steps: . resize by scale - to construct the next layer in the pyramid (size of image is drastically reduced) . | resize to keep image aspect-ratio (of the image in that pyramid layer, size the image will be slightly fit) . | . def image_pyramid(image, scale=1.5, minSize=(224, 224)): # yield the original image, this is the base of the image pyramid yield image # keep looping over the image pyramid while True: # compute the dimensions of the next image in the pyramid #scale controls how much the image is resized at each layer w = int(image.shape[1] / scale) # resize the image and take care of image aspect-ratio image = imutils.resize(image, width=w) # if the resized image does not meet the supplied minimum # size, then stop constructing the pyramid if image.shape[0] &lt; minSize[1] or image.shape[1] &lt; minSize[0]: break # yield the next image in the pyramid yield image . WIDTH = 600 # PYR_SCALE = 1.5 WIN_STEP = 16*3 # running on laptop so I generated a small pyramid ROI_SIZE = eval(args[&quot;size&quot;]) INPUT_SIZE = (224, 224) # input of resnet model.summary() . 5. Use ResNet trained with ImageNet for object detection . Load the pretrained model (any model). Have a look on what was images was trained on, and if its classification task is transferable to your classification task. Include top layer. . | Resize the images (size and aspect-ratio) to fit the size in the InputLayer in CNN. . | print(&quot;[INFO] loading network...&quot;) model = tf.keras.applications.resnet.ResNet50(weights = &quot;imagenet&quot;, include_top = &quot;True&quot;) print(&quot;...Done&quot;) . [INFO] loading network... ...Done . orig = image orig = imutils.resize(orig, width = WIDTH) (H, W) = orig.shape[:2] # 800, 600 . 6. Classification of ROIs . For each level in the pyramid run the sliding window. For each stop of the sliding window extract the window part of that image (ROI). Take the ROI and pass it trough the pre-trainied classifier. Look at the classification results, if fpr that ROI a classification result is greater than a minimum threshold, then record the class label and the position of the ROI/window in the original file name. . pyramid = image_pyramid(orig, scale=PYR_SCALE, minSize=ROI_SIZE) # initialize two lists, one to hold the ROIs generated from the image pyramid #and sliding window, and another list used to store the # (x, y)-coordinates of where the ROI was in the original image rois = [] locs = [] # time how long it takes to loop over the image pyramid layers and # sliding window locations start = time.time() counter = 0 tot_images = 0 for p, image in enumerate(pyramid): # determine the scale factor between the *original* image # dimensions and the *current* layer of the pyramid scale = W / float(image.shape[1]) # for each layer of the image pyramid, loop over the sliding # window locations sw = 0 for (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE): sw = sw + 1 # scale the (x, y)-coordinates of the ROI with respect to the # *original* image dimensions x = int(x * scale) y = int(y * scale) w = int(ROI_SIZE[0] * scale) h = int(ROI_SIZE[1] * scale) # take the ROI and pre-process it so we can later classify # the region using Keras/TensorFlow roi = cv2.resize(roiOrig, INPUT_SIZE) roi = tf.keras.preprocessing.image.img_to_array(roi) roi = tf.keras.applications.resnet.preprocess_input(roi) #print(roiOrig.shape, roi.shape) # update our list of ROIs and associated coordinates rois.append(roi) locs.append((x, y, x + w, y + h)) # check to see if we are visualizing each of the sliding # windows in the image pyramid if args[&quot;visualize&quot;] &gt; 0: # clone the original image and then draw a bounding box # surrounding the current region clone = orig.copy() cv2.rectangle(clone, (x, y), (x + w, y + h),(0, 255, 0), 5) # show the visualization and current ROI #plt.imshow(clone) #var_name = &quot;p&quot; + str(p)+&quot;_&quot; + &quot;sw&quot; + str(sw) + &quot;.jpg&quot; #plt.savefig(&quot;images/clone_&quot;+ var_name) #plt.imshow(roiOrig) #plt.savefig(&quot;images/roiOrig_&quot;+ var_name) #cv2.waitKey(0) tot_images = tot_images +1 print(roiOrig.shape, roi.shape) # show how long it took to loop over the image pyramid layers and # sliding window locations end = time.time() print(&quot;[INFO] looping over pyramid/windows took {:.5f} seconds&quot;.format(end - start)) print(&quot;Total images {:.2f}&quot;.format(tot_images)) . (150, 200, 3) (224, 224, 3) [INFO] looping over pyramid/windows took 0.21334 seconds Total images 176.00 . rois = np.array(rois, dtype=&quot;float32&quot;) # classify each of the proposal ROIs using ResNet and then show how # long the classifications took print(&quot;[INFO] classifying ROIs...&quot;) start = time.time() my_preds = model.predict(rois) end = time.time() print(&quot;[INFO] classifying ROIs took {:.5f} seconds&quot;.format(end - start)) . [INFO] classifying ROIs... [INFO] classifying ROIs took 48.74360 seconds . preds = tf.keras.applications.imagenet_utils.decode_predictions(my_preds, top=1) preds[30:35] . [[(&#39;n02085782&#39;, &#39;Japanese_spaniel&#39;, 0.33089334)], [(&#39;n02124075&#39;, &#39;Egyptian_cat&#39;, 0.6837845)], [(&#39;n02124075&#39;, &#39;Egyptian_cat&#39;, 0.75716674)], [(&#39;n02124075&#39;, &#39;Egyptian_cat&#39;, 0.43293664)], [(&#39;n02124075&#39;, &#39;Egyptian_cat&#39;, 0.6156667)]] . # labels (keys) to any ROIs associated with that label (values) #preds = tf.keras.applications.imagenet_utils.decode_predictions(my_preds, top=1) labels = {} #probs = {} # loop over the predictions for (i, p) in enumerate(preds): # grab the prediction information for the current ROI (imagenetID, label, prob) = p[0] # filter out weak detections by ensuring the predicted probability # is greater than the minimum probability if prob &gt;= args[&quot;min_conf&quot;]: # grab the bounding box associated with the prediction and # convert the coordinates box = locs[i] # grab the list of predictions for the label and add the # bounding box and probability to the list L = labels.get(label, []) L.append((box, prob)) labels[label] = L . allclone = orig.copy() for label in labels.keys(): # clone the original image so that we can draw on it print(&quot;[INFO] showing results for &#39;{}&#39;&quot;.format(label)) clone = orig.copy() # loop over all bounding boxes for the current label for (box, prob) in labels[label]: # draw the bounding box on the image (startX, startY, endX, endY) = box cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) # show the results *before* applying non-maxima suppression, then # clone the image again so we can display the results *after* # applying non-maxima suppression #plt.imshow(clone) #cv2.imshow(&quot;Before&quot;, clone) clone = orig.copy() # extract the bounding boxes and associated prediction # probabilities, then apply non-maxima suppression boxes = np.array([p[0] for p in labels[label]]) proba = np.array([p[1] for p in labels[label]]) boxes = non_max_suppression(boxes, proba) # loop over all bounding boxes that were kept after applying # non-maxima suppression for (startX, startY, endX, endY) in boxes: # draw the bounding box and label on the image cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2) cv2.rectangle(allclone, (startX, startY), (endX, endY),(0, 255, 0), 2) y = startY - 10 if startY - 10 &gt; 10 else startY + 10 cv2.putText(clone, label, (startX, y+30),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) cv2.putText(clone, &quot;{:.2f}&quot;.format(prob), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) cv2.putText(allclone, label, (startX, y+30),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) cv2.putText(allclone, &quot;{:.2f}&quot;.format(prob), (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2) # show the output after apply non-maxima suppression plt.imshow(clone) plt.imsave(&quot;images/keras_detection/_res03_&quot; + label + &quot;.jpg&quot;, clone) plt.imshow(allclone) plt.imsave(&quot;images/keras_detection/_allclone03.jpg&quot;, allclone) #plt.imshow(clone) #plt.imsave(&quot;images/_res03.jpg&quot;, clone) #cv2.imshow(&quot;After&quot;, clone) #cv2.waitKey(0) . [INFO] showing results for &#39;cowboy_boot&#39; [INFO] showing results for &#39;Egyptian_cat&#39; [INFO] showing results for &#39;guillotine&#39; [INFO] showing results for &#39;Arabian_camel&#39; . 7. The general flow of the algorithm . Input an image | Construct an image pyramid | For each scale of the image pyramid, run a sliding window . 3a. For each stop of the sliding window, extract the ROIs . 3b. Take the ROIs and pass it through our CNN originally trained for image classification . 3c. Examine the probability of the top class label of the CNN, and if meets a minimum confidence, record (1) the class label and (2) the location of the sliding window . | Apply class-wise non-maxima suppression to the bounding boxes . | Return results to calling function | References: . https https://www.pyimagesearch.com/2020/06/22/turning-any-cnn-image-classifier-into-an-object-detector-with-keras-tensorflow-and-opencv/&gt; https https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/ .",
            "url": "https://simsisim.github.io/sims-pyimage/keras/cnn/2020/12/23/Classifier2ObjectDetection.html",
            "relUrl": "/keras/cnn/2020/12/23/Classifier2ObjectDetection.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Similarity Between Images",
            "content": "import matplotlib.pyplot as plt import numpy as np import cv2 from skimage import metrics . 1. Load images . original_path = &quot;images/calculator_original.jpg&quot; contrast_path = &quot;images/calculator_contrast.jpg&quot; erased_path = &quot;images/calculator_erased.jpg&quot; failed_path = &quot;images/calculator_failed.jpg&quot; . img_original = cv2.imread(original_path) img_contrast = cv2.imread(contrast_path) img_erased = cv2.imread(erased_path) img_failed = cv2.imread(failed_path) img_original = cv2.cvtColor(img_original,cv2.COLOR_BGR2RGB) img_contrast = cv2.cvtColor(img_contrast, cv2.COLOR_BGR2RGB) img_erased = cv2.cvtColor(img_erased, cv2.COLOR_BGR2RGB) img_failed = cv2.cvtColor(img_failed, cv2.COLOR_BGR2RGB) . img_original.shape . (3968, 2976, 3) . #fig = plt.figure(figsize = (15, 15)) #ax1 = fig.add_subplot(131)# 1row + 3cols #ax1 = ax1.imshow(img_original) #plt.colorbar(ax1) #ax2 = fig.add_subplot(132) #ax2 = ax2.imshow(img_contrast) #plt.colorbar(ax2) #ax3 = fig.add_subplot(133) #ax3 = ax3.imshow(img_erased) #plt.colorbar(ax3) . . fig = plt.figure(figsize = (10,10)) images = (&quot;Original&quot;, img_original), (&quot;Contrast&quot;, img_contrast), (&quot;Erased&quot;, img_erased), (&quot;Failed&quot;, img_failed) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 4, i + 1) ax.set_title(name) plt.imshow(image, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) . 2. Compute MSE and SSI . def mse_img(imgA, imgB): # img_orig = original image # img = image that we want to compare against shape = imgA.shape # image shape (height, width, channels) acc = 0 # squared differences of all channels = 0 for i in range(shape[2]): # loop over channels in an img - RGB # calculate the squared differences for each channel in img squared_diff = np.square(imgA[:, :, i] - imgB[:, :, i]) # add squared differences of all channels # this res will be a matrix with same width x height as the original image, but with 1 channel acc = acc + squared_diff #compute the sum of diferences and divide by number of pixels x no of channels return np.sum(acc)/(shape[0] * shape[1]*shape[2]) def ssi_img(imgA, imgB): return metrics.structural_similarity(imgA, imgB, multichannel = True) . def compare_images(imageA, imageB, title): mse = mse_img(imageA, imageB) ssi = ssi_img(imageA, imageB) # setup the figure fig = plt.figure(title) plt.suptitle(&quot;MSE: %.2f, SSIM: %.2f&quot; % (mse, ssi)) # show first image ax = fig.add_subplot(1, 2, 1) plt.imshow(imageA, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) # show the second image ax = fig.add_subplot(1, 2, 2) plt.imshow(imageB, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) # show the images plt.show() . compare_images(img_original, img_original, &quot;Original vs. Original&quot;) . compare_images(img_original, img_contrast, &quot;Original vs. Contrast&quot;) . compare_images(img_original, img_erased, &quot;Original vs. Erased&quot;) . compare_images(img_original, img_failed, &quot;Original vs. Failed&quot;) . References: . https https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/ .",
            "url": "https://simsisim.github.io/sims-pyimage/image%20processing/computer%20vision/2020/12/22/Similarity_Images.html",
            "relUrl": "/image%20processing/computer%20vision/2020/12/22/Similarity_Images.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Day 2 Open CV Tutorial",
            "content": "# Convert jupyter notebook to python script: #!jupyter nbconvert --to script file-name.ipynb #$ python file-name.py --image01 image01.png --image02 image02.png --image03 image03.png # in ap.add_arrgument use : required = True + remove default path #args = vars(ap.parse_args()) . . import numpy as np import cv2 import argparse import matplotlib.pyplot as plt . ap = argparse.ArgumentParser(description=&#39;Fooo&#39;) ap.add_argument(&quot;-i1&quot;, &quot;--image01&quot;, default = &quot;images/face_detection05.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-i2&quot;, &quot;--image02&quot;, default = &quot;images/green.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-i3&quot;, &quot;--image03&quot;, default = &quot;images/tetris_blocks.png&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) args = vars(ap.parse_args([])) #for *.py use args = vars(ap.parse_args()) . 1. Loading / Displaying an image . image = args[&quot;image01&quot;] image = cv2.imread(image)#imread(args[&quot;image&quot;]) (h, w, c) = image.shape for i in range(1): fig = plt.figure(&quot;My Image&quot;, figsize = (7, 5)) ax = fig.add_subplot(111) plt.imshow(image, cmap = plt.cm.gray) plt.axis(&quot;off&quot;) #plt.suptitle(&quot;My Image&quot;) . #cv2.imshow(&quot;Image&quot;, image) #cv2.waitKey(0) plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7f1e5a6fb850&gt; . 2. Accesing Individual Pixels . # OpenCV stores images in BGR order rather than RGB (B, G, R) = image[100, 50] print(&quot;R={}, G={}, B={}&quot;.format(R, G, B)) . R=119, G=118, B=116 . 3. Array slicing/cropping . it is useful when extracting ROI (Region of Interest) | . # input image starting at x=320,y=60 at ending at x=420,y=160 roi = image[200:600, 0:500] plt.imshow(roi)#cv2.imshow(&quot;ROI&quot;, roi)cv2.waitKey(0) . &lt;matplotlib.image.AxesImage at 0x7f1e38247ad0&gt; . 4. Resizing an image . In the case of deep learning, we often resize images, ignoring aspect ratio, so that the volume fits into a network which requires that an image be square and of a certain dimension. | . resized = cv2.resize(image, (300, 300)) plt.imshow(resized) . &lt;matplotlib.image.AxesImage at 0x7f1e381860d0&gt; . #and calculate new height based on aspect-ratio in original image. r = 300.0 / w # ratio of old width /new width dim = (300, int(h * r)) resized = cv2.resize(image, dim) plt.imshow(resized) . &lt;matplotlib.image.AxesImage at 0x7f1e32db2710&gt; . 5. Rotating an image . Check imutils by pyimagesearch if you want to avoid croping the edges | . center = (w // 2, h // 2) #Rotating an image about the center point requires that we first calculate the center (x, y)-coordinates of the image #//to perform integer math #rotate image 45 deg clockwise M = cv2.getRotationMatrix2D(center, 45, 1.0) #we warp the image using the matrix (effectively rotating it) rotated = cv2.warpAffine(image, M, (w, h)) plt.imshow(rotated) #cv2.imshow(&quot;OpenCV Rotation&quot;, rotated) #cv2.waitKey(0) . &lt;matplotlib.image.AxesImage at 0x7f1e381a03d0&gt; . 6. Smoothing an image . Bluring an image reduces high-frequency noise, making it easier for NN algorithms to detect and understand the actual contents of the image rather than just noise that will “confuse” our algorithms. | . # larger kernel will give blurrier image blurred = cv2.GaussianBlur(image, (25, 25), 0) plt.imshow(blurred) . &lt;matplotlib.image.AxesImage at 0x7f1e3115ed90&gt; . 6. Drawing on an image . output = image.copy() # output: image to be drawn on # (x, y): top-left coordinates of rectangle # (x, y): right-bottom coordinates of rectangle cv2.rectangle(output, (200, 200), (600, 600), (255, 0, 240), 5) cv2.rectangle(output, (0, 200), (200, 400), (50, 205, 50), 5) cv2.circle(output, (1000, 700), 30, (255, 0, 0), -1) cv2.line(output, (800, 830), (800, 100), (255, 0, 0), 5) plt.imshow(output) . &lt;matplotlib.image.AxesImage at 0x7f1e30b6f810&gt; . output = image.copy() FONT_SIZE = 3 COLOR = (0, 255, 0) LINE_W = 2 LOC = (100, 100) PROB = 0.9 LOC2 = (200, 200) cv2.putText(output, &quot;1992: M + B + T&quot;, LOC, cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE, COLOR, LINE_W) cv2.putText(output, &quot;{:.2f}&quot;.format(PROB), LOC2, cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE, COLOR, LINE_W) plt.imshow(output) . &lt;matplotlib.image.AxesImage at 0x7fec79c6e850&gt; . 7.Converting an image to grayscale . channels = 1 | NN transform 1 channels to 3 for input in NN | . image = args[&quot;image02&quot;]#&quot;images/green.jpg&quot; image = cv2.imread(image) norm = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) . #fig = plt.figure(figsize = (10, 10)) #ax = fig.add_subplot(131) #ax.set_title(&quot;BGR&quot;) #plt.imshow(image) ##plt.axis(&quot;off&quot;) #ax = fig.add_subplot(132) #plt.imshow(norm) #plt.axis(&quot;off&quot;) #ax.set_title(&quot;RGB&quot;) ##plt.colorbar() #ax = fig.add_subplot(133) #ax.set_title(&quot;GRAY&quot;) #plt.imshow(gray) ##plt.axis(&quot;off&quot;) ##plt.colorbar() . . fig = plt.figure(figsize = (10,10)) images = (&quot;BGR&quot;, image), (&quot;RGB&quot;, norm), (&quot;GRAY&quot;, gray) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 3, i + 1) ax.set_title(name) plt.imshow(image, cmap = &quot;gray&quot;) plt.axis(&quot;off&quot;) . 8. Perform edge detection . image = args[&quot;image03&quot;] image = cv2.imread(image) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) plt.imshow(gray, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e180c8ed0&gt; . edged_d = cv2.Canny(gray, 4, 150) edged_c = cv2.Canny(gray, 100, 150) . images = (&quot;Coarse&quot;, edged_c), (&quot;Detailed&quot;, edged_d) fig = plt.figure(figsize = (10, 10)) for i , (name, image) in enumerate(images): ax = fig.add_subplot(1,2, i+1) ax.set_title(name) plt.imshow(image, cmap = &quot;gray&quot;) plt.axis(&quot;off&quot;) . 9. Thresholding a grayscale image . thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)[1] plt.imshow(thresh, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e18380b10&gt; . 10. Erosions and dilations . Errode away pixels to make smoother edges | . mask = thresh.copy() mask = cv2.erode(mask, None, iterations=5) plt.imshow(mask, cmap = &quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x7f1e18029150&gt; . 11.Detecting and drawing contours . to be continued | . 12. Save a file name . i = 0 plt.imshow(image) filename = &quot;images/cv2_tut/my_image_&quot;+&quot;{:02d}&quot;.format(i) +&quot;.jpg&quot; cv2.imwrite(filename, image) . True . References . https https://www.pyimagesearch.com/2018/07/19/opencv-tutorial-a-guide-to-learn-opencv/ .",
            "url": "https://simsisim.github.io/sims-pyimage/computer%20vision/image%20processing/opencv/2020/12/22/Day2-OpenCVTutorial.html",
            "relUrl": "/computer%20vision/image%20processing/opencv/2020/12/22/Day2-OpenCVTutorial.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Day 1 Face Detection with OpenCV and Deep Learning",
            "content": "# Convert jupyter notebook to python script: #!jupyter nbconvert --to script file-name.ipynb #$ python file-name.py --image01 image01.png etc # in ap.add_arrgument use : required = True + remove default path #args = vars(ap.parse_args()) . . import numpy as np import argparse import cv2 import os import matplotlib.pyplot as plt . #ap.add_argument(&#39;--name&#39;, &#39;-n&#39;, default=&#39;foo&#39;, help=&#39;foo&#39;) ap = argparse.ArgumentParser(description=&#39;Fooo&#39;) ap.add_argument(&quot;-i&quot;, &quot;--image&quot;, default = &quot;images/face_detection05.jpg&quot;, required=False,# for *.py use required= True help=&quot;path to input image&quot;) ap.add_argument(&quot;-p&quot;, &quot;--prototxt&quot;, default = &quot;models/deploy.prototxt.txt&quot;, help=&quot;path to Caffe &#39;deploy&#39; prototxt file&quot;) ap.add_argument(&quot;-m&quot;, &quot;--model&quot;, default = &quot;models/res10_300x300_ssd_iter_140000.caffemodel&quot;,#required=True, help=&quot;path to Caffe pre-trained model&quot;) ap.add_argument(&quot;-c&quot;, &quot;--confidence&quot;, type=float, default=0.90, help=&quot;minimum probability to filter weak detections&quot;) args = vars(ap.parse_args([])) #for *.py use args = vars(ap.parse_args()) . # by resizing to a fixed 300x300 pixels and then normalizing it image = cv2.imread(args[&quot;image&quot;]) (h, w) = image.shape[:2] blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0)) print(h, w) . 870 1315 . print(&quot;[INFO] loading model...&quot;) net = cv2.dnn.readNetFromCaffe(args[&quot;prototxt&quot;], args[&quot;model&quot;]) print(&quot;...done&quot;) . [INFO] loading model... ...done . # predictions print(&quot;[INFO] computing object detections...&quot;) net.setInput(blob) detections = net.forward() print(&quot;...done&quot;) . [INFO] computing object detections... ...done . for i in range(0, detections.shape[2]): # extract the confidence (i.e., probability) associated with the # prediction confidence = detections[0, 0, i, 2] # filter out weak detections by ensuring the `confidence` is # greater than the minimum confidence if confidence &gt; args[&quot;confidence&quot;]: # compute the (x, y)-coordinates of the bounding box for the # object box = detections[0, 0, i, 3:7] * np.array([w, h, w, h]) (startX, startY, endX, endY) = box.astype(&quot;int&quot;) # draw the bounding box of the face along with the associated # probability text = &quot;{:.2f}%&quot;.format(confidence * 100) y = startY - 10 if startY - 10 &gt; 10 else startY + 10 cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2) cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2) # show the output image plt.imshow(image) cv2.imwrite(&quot;images/face_detection05_t09.jpg&quot;, image) #cv2.waitKey(0) . True . Tune model by varying threshold for edge detection. . fig = plt.figure(figsize = (10,10)) image_th02 = &quot;images/face_detection05_t02.jpg&quot; #image_th05 = &quot;face_detection05_t05.jpg&quot; image_th09 = &quot;images/face_detection05_t09.jpg&quot; image_th02 = cv2.imread(image_th02) #image_th05 = &quot;face_detection05_t05.jpg&quot; image_th09 = cv2.imread(image_th09) #print(image) images = (&quot;th = 0.2&quot;, image_th02), (&quot;th = 0.9&quot;, image_th09) # loop over the images for (i, (name, image)) in enumerate(images): # show the image ax = fig.add_subplot(1, 2, i + 1) ax.set_title(name) plt.imshow(image) plt.axis(&quot;off&quot;) . References: . https https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/ . https https://realpython.com/command-line-interfaces-python-argparse/ . https https://medium.com/@data.scientist/ipython-trick-how-to-use-argparse-in-ipython-notebooks-a07423ab31fc . https https://www.pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/ .",
            "url": "https://simsisim.github.io/sims-pyimage/deep%20learning/computer%20vision/2020/12/22/Day1_Face_Detection_OpenCV.html",
            "relUrl": "/deep%20learning/computer%20vision/2020/12/22/Day1_Face_Detection_OpenCV.html",
            "date": " • Dec 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://simsisim.github.io/sims-pyimage/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://simsisim.github.io/sims-pyimage/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://simsisim.github.io/sims-pyimage/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://simsisim.github.io/sims-pyimage/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}